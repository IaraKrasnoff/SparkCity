{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Environment Setup & Data Exploration\n",
    "# Smart City IoT Analytics Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ LEARNING OBJECTIVES:\n",
    "- Configure Spark cluster and development environment\n",
    "- Understand IoT data characteristics and challenges  \n",
    "- Implement basic data ingestion patterns\n",
    "- Explore PySpark DataFrame operations\n",
    "\n",
    "## üìÖ SCHEDULE:\n",
    "**Morning (4 hours):**\n",
    "1. Environment Setup (2 hours)\n",
    "2. Data Exploration (2 hours)\n",
    "\n",
    "**Afternoon (4 hours):**  \n",
    "3. Basic Data Ingestion (2 hours)\n",
    "4. Initial Data Transformations (2 hours)\n",
    "\n",
    "## ‚úÖ DELIVERABLES:\n",
    "- Working Spark cluster with all services running\n",
    "- Data ingestion notebook with basic EDA\n",
    "- Documentation of data quality findings  \n",
    "- Initial data loading pipeline functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Welcome to the Smart City IoT Analytics Pipeline!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Welcome to the Smart City IoT Analytics Pipeline!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 1: ENVIRONMENT SETUP (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.1: Initialize Spark Session (15 minutes)\n",
    "\n",
    "üéØ **TASK:** Create a Spark session configured for local development  \n",
    "üí° **HINT:** Use SparkSession.builder with appropriate configurations  \n",
    "üìö **DOCS:** https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "**TODO:** Create Spark session with the following configurations:\n",
    "- App name: \"SmartCityIoTPipeline-Day1\"\n",
    "- Master: \"local[*]\" (use all available cores)\n",
    "- Memory: \"4g\" for driver\n",
    "- Additional configs for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/05 09:08:27 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-29.local, resolves to a loopback address: 127.0.0.1; using 192.168.87.88 instead (on interface en0)\n",
      "25/09/05 09:08:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/09/05 09:08:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Details:\n",
      "   App Name: SmartCityIoTPipeline-Day1\n",
      "   Spark Version: 4.0.0\n",
      "   Master: local[*]\n",
      "   Default Parallelism: 8\n"
     ]
    }
   ],
   "source": [
    "# Create Spark session with the following configurations:\n",
    "jdbc_jar_path = \"/Users/iara/Projects/SparkCity/postgresql-42.7.7.jar\"  # Update this path as necessary\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"SmartCityIoTPipeline-Day1\")  # App name for identification\n",
    "         .master(\"local[*]\")   # Use all available CPU cores\n",
    "         .config(\"spark.driver.memory\", \"4g\")  # Set driver memory to 4GB\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "         .config(\"spark.jars\", jdbc_jar_path)  # Include JDBC driver\n",
    "         .getOrCreate())\n",
    "\n",
    "# TODO: Verify Spark session is working\n",
    "print(\"‚úÖ Spark Session Details:\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.2: Verify Infrastructure (15 minutes)\n",
    "\n",
    "üéØ **TASK:** Check that all infrastructure services are running  \n",
    "üí° **HINT:** Test database connectivity and file system access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking Infrastructure...\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£ Docker Services Check:\n",
      "üê≥ Docker Services Status:\n",
      "NAMES            STATUS\n",
      "sparkcity-db-1   Up 27 minutes\n",
      "\n",
      "‚úÖ PostgreSQL container appears to be running\n",
      "\n",
      "2Ô∏è‚É£ Database Connection Test:\n",
      "   Trying Docker PostgreSQL...\n",
      "‚úÖ Database connection successful to Docker PostgreSQL!\n",
      "   Test query result: [Row(test_column=1)]\n",
      "\n",
      "4Ô∏è‚É£ Spark Web UI:\n",
      "üåê Spark UI should be accessible at: http://localhost:4040\n",
      "   (Open this in your browser to monitor Spark jobs)\n",
      "\n",
      "============================================================\n",
      "üìä INFRASTRUCTURE STATUS SUMMARY\n",
      "============================================================\n",
      "   Docker Services: ‚úÖ Running\n",
      "   Database Connection: ‚úÖ Connected (postgresql)\n",
      "   Alternative Data: ‚úÖ Ready\n",
      "   Spark Session: ‚úÖ Active\n",
      "\n",
      "üéâ READY TO PROCEED!\n",
      "   Infrastructure is set up for the lab exercises\n"
     ]
    }
   ],
   "source": [
    "# Check infrastructure and provide alternatives for environments without Docker\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def check_docker_services():\n",
    "    \"\"\"Check if Docker services are running\"\"\"\n",
    "    try:\n",
    "        # Check if docker-compose services are running\n",
    "        result = subprocess.run(['docker', 'ps', '--format', 'table {{.Names}}\\t{{.Status}}'], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        \n",
    "        print(\"üê≥ Docker Services Status:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Look for common service names\n",
    "        running_services = result.stdout.lower()\n",
    "        postgres_running = 'postgres' in running_services or 'db' in running_services\n",
    "        \n",
    "        if postgres_running:\n",
    "            print(\"‚úÖ PostgreSQL container appears to be running\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå PostgreSQL container not found\")\n",
    "            print(\"üí° Start services with: docker-compose up -d\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"‚ùå Docker is not running or docker-compose services are not started\")\n",
    "        print(\"üí° Make sure Docker is running and start services with: docker-compose up -d\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Docker command not found - this is normal in many notebook environments\")\n",
    "        print(\"üí° Alternative approaches:\")\n",
    "        print(\"   1. Use local PostgreSQL installation\")\n",
    "        print(\"   2. Use SQLite for local development\")\n",
    "        print(\"   3. Work with file-based data sources only\")\n",
    "        print(\"   4. Use cloud database services\")\n",
    "        return False\n",
    "\n",
    "def test_database_connection_with_fallback():\n",
    "    \"\"\"Test database connection with fallback options\"\"\"\n",
    "    \n",
    "    # First try PostgreSQL (Docker or local)\n",
    "    postgres_configs = [\n",
    "        {\n",
    "            \"url\": \"jdbc:postgresql://localhost:5432/smartcity\",\n",
    "            \"user\": \"postgres\", \n",
    "            \"password\": \"password\",\n",
    "            \"description\": \"Docker PostgreSQL\"\n",
    "        },\n",
    "        {\n",
    "            \"url\": \"jdbc:postgresql://localhost:5432/postgres\", \n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"\",\n",
    "            \"description\": \"Local PostgreSQL (default)\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    db_properties_base = {\"driver\": \"org.postgresql.Driver\"}\n",
    "    \n",
    "    for config in postgres_configs:\n",
    "        try:\n",
    "            db_properties = {**db_properties_base, \"user\": config[\"user\"], \"password\": config[\"password\"]}\n",
    "            \n",
    "            print(f\"   Trying {config['description']}...\")\n",
    "            test_df = spark.read.jdbc(\n",
    "                url=config[\"url\"],\n",
    "                table=\"(SELECT 1 as test_column) as test_table\",\n",
    "                properties=db_properties\n",
    "            )\n",
    "            \n",
    "            result = test_df.collect()\n",
    "            print(f\"‚úÖ Database connection successful to {config['description']}!\")\n",
    "            print(f\"   Test query result: {result}\")\n",
    "            return True, \"postgresql\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed to connect to {config['description']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # If PostgreSQL fails, suggest alternatives\n",
    "    print(\"\\nüí° Database connection alternatives:\")\n",
    "    print(\"   1. File-based approach: Work with CSV/JSON/Parquet files only\")\n",
    "    print(\"   2. SQLite: Use embedded database for local development\") \n",
    "    print(\"   3. In-memory: Create sample data directly in Spark DataFrames\")\n",
    "    print(\"   4. Cloud databases: Use managed database services\")\n",
    "    \n",
    "    return False, \"none\"\n",
    "\n",
    "def create_sample_data_alternative():\n",
    "    \"\"\"Create sample data directly in Spark if no database is available\"\"\"\n",
    "    print(\"\\nüîß Creating sample data alternative...\")\n",
    "    \n",
    "    try:\n",
    "        # Create sample zones data\n",
    "        zones_data = [\n",
    "            (\"zone_001\", \"Downtown\", \"commercial\", 40.7589, 40.7789, -73.9851, -73.9651),\n",
    "            (\"zone_002\", \"Residential North\", \"residential\", 40.7789, 40.7989, -73.9851, -73.9651),\n",
    "            (\"zone_003\", \"Industrial South\", \"industrial\", 40.7389, 40.7589, -73.9851, -73.9651)\n",
    "        ]\n",
    "        \n",
    "        zones_schema = [\"zone_id\", \"zone_name\", \"zone_type\", \"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\"]\n",
    "        sample_zones_df = spark.createDataFrame(zones_data, zones_schema)\n",
    "        \n",
    "        print(\"‚úÖ Created sample zones data in memory\")\n",
    "        sample_zones_df.show()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating sample data: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Check infrastructure step by step\n",
    "print(\"üîç Checking Infrastructure...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Check Docker\n",
    "print(\"\\n1Ô∏è‚É£ Docker Services Check:\")\n",
    "docker_running = check_docker_services()\n",
    "\n",
    "# Step 2: Test Database\n",
    "print(\"\\n2Ô∏è‚É£ Database Connection Test:\")\n",
    "db_connected, db_type = test_database_connection_with_fallback()\n",
    "\n",
    "# Step 3: Alternative data approach if no database\n",
    "if not db_connected:\n",
    "    print(\"\\n3Ô∏è‚É£ Alternative Data Setup:\")\n",
    "    alt_data_ready = create_sample_data_alternative()\n",
    "else:\n",
    "    alt_data_ready = True\n",
    "\n",
    "# Step 4: Spark UI check\n",
    "print(\"\\n4Ô∏è‚É£ Spark Web UI:\")\n",
    "print(\"üåê Spark UI should be accessible at: http://localhost:4040\")\n",
    "print(\"   (Open this in your browser to monitor Spark jobs)\")\n",
    "\n",
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä INFRASTRUCTURE STATUS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Docker Services: {'‚úÖ Running' if docker_running else '‚ùå Not Available'}\")\n",
    "print(f\"   Database Connection: {'‚úÖ Connected (' + db_type + ')' if db_connected else '‚ùå Failed'}\")\n",
    "print(f\"   Alternative Data: {'‚úÖ Ready' if alt_data_ready else '‚ùå Not Ready'}\")\n",
    "print(f\"   Spark Session: {'‚úÖ Active' if 'spark' in locals() else '‚ùå Not Created'}\")\n",
    "\n",
    "if not docker_running and not db_connected:\n",
    "    print(\"\\nüí° RECOMMENDATION:\")\n",
    "    print(\"   Continue with file-based data approach\")\n",
    "    print(\"   This notebook can work without Docker/PostgreSQL\")\n",
    "    print(\"   Focus on PySpark DataFrame operations with CSV/JSON/Parquet files\")\n",
    "else:\n",
    "    print(\"\\nüéâ READY TO PROCEED!\")\n",
    "    print(\"   Infrastructure is set up for the lab exercises\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1.3: Generate Sample Data (30 minutes)\n",
    "\n",
    "üéØ **TASK:** Run the data generation script to create sample IoT data  \n",
    "üí° **HINT:** Use the provided data generation script or run it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating data... (0/5 files exist)\n",
      "   Project root: /Users/iara/Projects/SparkCity\n",
      "   Running script from: /Users/iara/Projects/SparkCity\n",
      "‚úÖ Data generation successful!\n"
     ]
    }
   ],
   "source": [
    "# Generate Sample IoT Data\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def check_and_generate_data():\n",
    "    \"\"\"Check if data exists, generate if missing\"\"\"\n",
    "    data_files = [\"traffic_sensors.csv\", \"air_quality.json\", \"weather_data.parquet\", \n",
    "                  \"energy_meters.csv\", \"city_zones.csv\"]\n",
    "    data_path = \"data/raw\"\n",
    "    \n",
    "    # Check existing files\n",
    "    existing = [f for f in data_files if os.path.exists(f\"{data_path}/{f}\")]\n",
    "    \n",
    "    if len(existing) == len(data_files):\n",
    "        print(f\"‚úÖ All {len(data_files)} data files found!\")\n",
    "        return True\n",
    "    \n",
    "    # Generate missing data\n",
    "    print(f\"üîÑ Generating data... ({len(existing)}/{len(data_files)} files exist)\")\n",
    "    \n",
    "    try:\n",
    "        # Get the project root (go up one level from notebooks folder)\n",
    "        notebook_dir = os.getcwd()  # Current directory (notebooks/)\n",
    "        project_root = os.path.dirname(notebook_dir)  # Go up one level to SparkCity/\n",
    "        \n",
    "        print(f\"   Project root: {project_root}\")\n",
    "        print(f\"   Running script from: {project_root}\")\n",
    "        \n",
    "        # Run from project root directory\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"scripts/generate_data.py\"], \n",
    "            cwd=project_root,  # Run from SparkCity/ directory\n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=300\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Data generation successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Generation failed!\")\n",
    "            if result.stderr:\n",
    "                print(f\"   Error: {result.stderr.strip()}\")\n",
    "            if result.stdout:\n",
    "                print(f\"   Output: {result.stdout.strip()}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run data check/generation\n",
    "data_ready = check_and_generate_data()\n",
    "\n",
    "# If failed, provide clear manual instructions\n",
    "if not data_ready:\n",
    "    print(\"\\nüîß MANUAL FIX:\")\n",
    "    print(\"1. Open terminal\")\n",
    "    print(\"2. Run: cd /Users/sai/Documents/Projects/ninth-week/SparkCity\")\n",
    "    print(\"3. Run: python scripts/generate_data.py\")\n",
    "    print(\"4. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 2: DATA EXPLORATION (Morning - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä SECTION 2: EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä SECTION 2: EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.1: Load and Examine Data Sources (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Load each data source and examine its structure  \n",
    "üí° **HINT:** Use appropriate Spark readers for different file formats  \n",
    "üìö **CONCEPTS:** Schema inference, file formats, data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = \"../data/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Loading City Zones Reference Data...\n",
      "   üìä Records: 8\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- zone_id: string (nullable = true)\n",
      " |-- zone_name: string (nullable = true)\n",
      " |-- zone_type: string (nullable = true)\n",
      " |-- lat_min: double (nullable = true)\n",
      " |-- lat_max: double (nullable = true)\n",
      " |-- lon_min: double (nullable = true)\n",
      " |-- lon_max: double (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|zone_id |zone_name         |zone_type  |lat_min|lat_max|lon_min|lon_max|population|\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "|ZONE_001|Downtown          |commercial |40.72  |40.74  |-74.01 |-73.99 |25000     |\n",
      "|ZONE_002|Financial District|commercial |40.7   |40.72  |-74.02 |-74.0  |15000     |\n",
      "|ZONE_003|Residential North |residential|40.76  |40.8   |-74.0  |-73.98 |45000     |\n",
      "|ZONE_004|Residential South |residential|40.7   |40.72  |-73.98 |-73.96 |38000     |\n",
      "|ZONE_005|Industrial Park   |industrial |40.74  |40.76  |-74.02 |-74.0  |5000      |\n",
      "+--------+------------------+-----------+-------+-------+-------+-------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load city zones reference data\n",
    "print(\"üìç Loading City Zones Reference Data...\")\n",
    "try:\n",
    "    zones_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/city_zones.csv\")\n",
    "    \n",
    "    # TODO: Display basic information about zones\n",
    "    print(f\"   üìä Records: {zones_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    zones_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    zones_df.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading zones data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöó Loading Traffic Sensors Data...\n",
      "   üìä Records: 100850\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      " |-- road_type: string (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+----------+\n",
      "|  sensor_id|           timestamp|      location_lat|      location_lon|vehicle_count|         avg_speed|congestion_level| road_type|\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+----------+\n",
      "|TRAFFIC_001|2025-08-29 09:08:...|  40.7637107146774|-73.93931897855427|           21|              10.0|          medium|commercial|\n",
      "|TRAFFIC_002|2025-08-29 09:08:...| 40.74316378780784|-74.00032283795375|           29|25.019032751217928|            high|  arterial|\n",
      "|TRAFFIC_003|2025-08-29 09:08:...|40.795479614720705| -73.9100710677773|           28| 32.54907485592873|            high|commercial|\n",
      "|TRAFFIC_004|2025-08-29 09:08:...|40.792385250140825|-73.97932215692458|           34|35.490927554052526|            high|  arterial|\n",
      "|TRAFFIC_005|2025-08-29 09:08:...| 40.72834458387762|-73.97707742647704|           24| 20.86875431580467|            high|commercial|\n",
      "+-----------+--------------------+------------------+------------------+-------------+------------------+----------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load traffic sensors data  \n",
    "print(\"\\nüöó Loading Traffic Sensors Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file with proper options\n",
    "    traffic_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/traffic_sensors.csv\")\n",
    "\n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {traffic_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    traffic_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    traffic_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading traffic data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå´Ô∏è Loading Air Quality Data...\n",
      "   üìä Records: 13460\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- co: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- no2: double (nullable = true)\n",
      " |-- pm10: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+---------+------------------+--------------------+\n",
      "|                co|         humidity|      location_lat|      location_lon|               no2|             pm10|              pm25|sensor_id|       temperature|           timestamp|\n",
      "+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+---------+------------------+--------------------+\n",
      "|1.6335785026294862| 44.8763468773436|40.789142894254475|-73.91141431648252| 36.62880854500891|65.63436132517162| 21.35894617830246|   AQ_001|15.281212637178804|2025-08-29T09:08:...|\n",
      "| 2.005684030999196|70.14933878214755| 40.77203955361523|-73.94472111705353| 36.02558738282434|56.38879977307145| 27.30541645168328|   AQ_002| 20.17930009172589|2025-08-29T09:08:...|\n",
      "|1.0316095775048042|50.07643893680009| 40.79296019215918|-73.94629414459922|26.650586965714105| 66.2814622104104|25.810639760446037|   AQ_003|15.620067677229276|2025-08-29T09:08:...|\n",
      "|1.5999082352442031| 68.1048227225385|40.746917874554235|-73.91992269858642| 48.78912369679307| 69.1255510717119| 30.04490635944618|   AQ_004|19.946891459531486|2025-08-29T09:08:...|\n",
      "|1.7818798191622844| 65.1637079991697| 40.77554866405218| -73.9399940174617| 50.22602381159401|  39.308805260836|24.534511803769263|   AQ_005|  8.56606592060349|2025-08-29T09:08:...|\n",
      "+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+---------+------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load air quality data (JSON format)\n",
    "print(\"\\nüå´Ô∏è Loading Air Quality Data...\")\n",
    "try:\n",
    "    # TODO: Load JSON file - note different file format!\n",
    "    air_quality_df = spark.read.option(\"multiline\", \"true\").json(f\"{data_dir}/air_quality.json\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {air_quality_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    air_quality_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    air_quality_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading air quality data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå§Ô∏è Loading Weather Data...\n",
      "   üìä Records: 3370\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-------------+------------------+\n",
      "| station_id|           timestamp|      location_lat|      location_lon|       temperature|         humidity|        wind_speed|    wind_direction|precipitation|          pressure|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-------------+------------------+\n",
      "|WEATHER_001|2025-08-29T09:08:...| 40.78296077142171| -73.9934962236705|24.984236133556845|78.15054647781534| 4.185372192987507|208.60990650213705|          0.0|  996.910492047061|\n",
      "|WEATHER_002|2025-08-29T09:08:...|40.710562777643034|-73.98716298503285|23.914353441028034|56.17979657005276| 6.752098729679913|35.834294369092845|          0.0|1025.2542196479658|\n",
      "|WEATHER_003|2025-08-29T09:08:...|40.773824659434695| -73.9289103873928|26.740857097898864|69.07139652292112| 6.711344552315434| 319.7417290051477|          0.0|1011.5597671591293|\n",
      "|WEATHER_004|2025-08-29T09:08:...|40.769766260234775|-73.90671545423301|24.168345033645863|52.18574455077462| 12.25789025802377|329.23658653851805|          0.0|1014.2960706345783|\n",
      "|WEATHER_005|2025-08-29T09:08:...| 40.72516565800465| -74.0123991821182|24.957857841287563|87.12399203301761|1.3812964778846188| 134.8191643029982|          0.0|1026.9242671851378|\n",
      "+-----------+--------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load weather data (Parquet format)\n",
    "print(\"\\nüå§Ô∏è Loading Weather Data...\")\n",
    "try:\n",
    "    # TODO: Load Parquet file - another different format!\n",
    "    weather_df = spark.read.parquet(f\"{data_dir}/weather_data.parquet\")\n",
    "\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {weather_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    weather_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    weather_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading weather data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Loading Energy Meters Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Records: 201800\n",
      "   üìã Schema:\n",
      "root\n",
      " |-- meter_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- power_consumption: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- current: double (nullable = true)\n",
      " |-- power_factor: double (nullable = true)\n",
      "\n",
      "   üîç Sample Data:\n",
      "+-----------+--------------------+-------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|   meter_id|           timestamp|building_type|     location_lat|      location_lon| power_consumption|           voltage|           current|      power_factor|\n",
      "+-----------+--------------------+-------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|ENERGY_0001|2025-08-29 09:08:...|  residential|40.72114375856278|-74.00249816601261|1.8875247358734946|236.78468194392684|  7.97148160251549|0.9004311718787728|\n",
      "|ENERGY_0002|2025-08-29 09:08:...|   commercial|40.72805593323459|-73.96336795104845|25.978482711365046|246.65013188051853|105.32523341179262|0.8683348821275563|\n",
      "|ENERGY_0003|2025-08-29 09:08:...|  residential| 40.7249142509706|-73.91070063378747|3.0105667988507236|240.08982788553178|12.539335070397398|0.9351900140825554|\n",
      "|ENERGY_0004|2025-08-29 09:08:...|       office|40.77206099875656|-74.00887820741188|26.371029800147543|240.04917064699976|109.85678362924658|0.8643532275859439|\n",
      "|ENERGY_0005|2025-08-29 09:08:...|   commercial|40.72559054174745|  -73.991405585055|22.415241815384398|237.41176212959633| 94.41504335892405|0.9112549405592869|\n",
      "+-----------+--------------------+-------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load energy meters data\n",
    "print(\"\\n‚ö° Loading Energy Meters Data...\")\n",
    "try:\n",
    "    # TODO: Load CSV file\n",
    "    energy_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{data_dir}/energy_meters.csv\")\n",
    "    \n",
    "    # TODO: Display basic information\n",
    "    print(f\"   üìä Records: {energy_df.count()}\")\n",
    "    print(f\"   üìã Schema:\")\n",
    "    energy_df.printSchema()\n",
    "    \n",
    "    # TODO: Show sample data\n",
    "    print(f\"   üîç Sample Data:\")\n",
    "    energy_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading energy data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.2: Basic Data Quality Assessment (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Assess data quality across all datasets  \n",
    "üí° **HINT:** Check for missing values, duplicates, data ranges  \n",
    "üìö **CONCEPTS:** Data profiling, quality metrics, anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Perform basic data quality assessment on a DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to assess\n",
    "        dataset_name: Name of the dataset for reporting\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìã Data Quality Assessment: {dataset_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # TODO: Basic statistics\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    print(f\"   üìä Dimensions: {total_rows:,} rows √ó {total_cols} columns\")\n",
    "    \n",
    "    # TODO: Check for missing values\n",
    "    print(f\"   üîç Missing Values:\")\n",
    "    for col in df.columns:\n",
    "        missing_count = df.filter(F.col(col).isNull()).count()\n",
    "        missing_pct = (missing_count / total_rows) * 100\n",
    "        if missing_count > 0:\n",
    "            print(f\"      {col}: {missing_count:,} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    # TODO: Check for duplicate records\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"   üîÑ Duplicate Records: {duplicate_count:,}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No duplicate records found\")\n",
    "    \n",
    "    # TODO: Numeric column statistics\n",
    "    numeric_cols = [field.name for field in df.schema.fields \n",
    "                   if field.dataType in [IntegerType(), DoubleType(), FloatType(), LongType()]]\n",
    "    \n",
    "    if numeric_cols:\n",
    "        print(f\"   üìà Numeric Columns Summary:\")\n",
    "        # Show basic statistics for numeric columns\n",
    "        df.select(numeric_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Data Quality Assessment: City Zones\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 8 rows √ó 8 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/05 09:21:00 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|summary|             lat_min|             lat_max|            lon_min|            lon_max|        population|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "|  count|                   8|                   8|                  8|                  8|                 8|\n",
      "|   mean|  40.730000000000004|             40.7525| -73.99125000000001|          -73.97125|           21250.0|\n",
      "| stddev|0.023904572186687328|0.028157719063465373|0.02474873734153055|0.02474873734153458|14260.334598358582|\n",
      "|    min|                40.7|               40.72|             -74.02|              -74.0|              5000|\n",
      "|    max|               40.76|                40.8|             -73.96|             -73.94|             45000|\n",
      "+-------+--------------------+--------------------+-------------------+-------------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Traffic Sensors\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 100,850 rows √ó 8 columns\n",
      "   üîç Missing Values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|summary|        location_lat|       location_lon|     vehicle_count|         avg_speed|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "|  count|              100850|             100850|            100850|            100850|\n",
      "|   mean|  40.742328699349464| -73.95860675305708|  22.8747149231532| 45.59710859159847|\n",
      "| stddev|0.027254274819054226|0.03450187503062409|14.057453339709184| 17.03335457392023|\n",
      "|    min|  40.702130075705746| -74.01768379163737|                 0|               5.0|\n",
      "|    max|  40.795479614720705| -73.90298118535486|                90|110.34975001057589|\n",
      "+-------+--------------------+-------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Air Quality\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 13,460 rows √ó 10 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+-------------------+------------------+-------------------+-------------------+------------------+-----------------+-----------------+-------------------+\n",
      "|summary|                 co|          humidity|       location_lat|       location_lon|               no2|             pm10|             pm25|        temperature|\n",
      "+-------+-------------------+------------------+-------------------+-------------------+------------------+-----------------+-----------------+-------------------+\n",
      "|  count|              13460|             13460|              13460|              13460|             13460|            13460|            13460|              13460|\n",
      "|   mean| 1.2838683715109438| 54.99400751180326|  40.74451341961091| -73.94244827215097| 32.27107902964541| 43.0046894845401|26.87448808088201| 20.063499224634864|\n",
      "| stddev|0.42586720391367555|14.383480263158374|0.03309164312198147|0.03192350808042907|10.719774974704375| 13.0632596876881|8.644702952500275|  7.971679439508375|\n",
      "|    min|                0.0|30.000008778935456|  40.70257033454023| -74.01465211015366|               0.0|              0.0|              0.0|-11.513972827759684|\n",
      "|    max| 2.9456488851180644| 79.99662972788863|  40.79905731886954| -73.90919716354647|  76.1877819243618|93.23667568399492|58.80420530659306|  52.41970127332892|\n",
      "+-------+-------------------+------------------+-------------------+-------------------+------------------+-----------------+-----------------+-------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Weather Stations\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 3,370 rows √ó 10 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+--------------------+--------------------+------------------+-----------------+--------------------+--------------------+--------------------+------------------+\n",
      "|summary|        location_lat|        location_lon|       temperature|         humidity|          wind_speed|      wind_direction|       precipitation|          pressure|\n",
      "+-------+--------------------+--------------------+------------------+-----------------+--------------------+--------------------+--------------------+------------------+\n",
      "|  count|                3370|                3370|              3370|             3370|                3370|                3370|                3370|              3370|\n",
      "|   mean|   40.75091983153217|  -73.95856168866678|20.083263387003104| 59.7615884809405|    7.89301531121726|  181.51669231046858|0.052632743493562756|1013.7074345782222|\n",
      "| stddev|0.027993456741457434|0.033645024972745866| 3.307055186801816|14.58871468773441|   7.901249029901483|  102.87300543582232|  0.2181030300630195| 10.01759774668259|\n",
      "|    min|  40.710562777643034|   -74.0123991821182| 9.985113679183227|             20.0|0.002181882240607...|0.014555239978046863|                 0.0| 972.6557416983109|\n",
      "|    max|   40.78859089388549|  -73.90671545423301| 29.90597164657355|            100.0|   73.98874215221728|   359.9871029026818|  2.7683593458897455|1050.7551361651954|\n",
      "+-------+--------------------+--------------------+------------------+-----------------+--------------------+--------------------+--------------------+------------------+\n",
      "\n",
      "\n",
      "üìã Data Quality Assessment: Energy Meters\n",
      "--------------------------------------------------\n",
      "   üìä Dimensions: 201,800 rows √ó 9 columns\n",
      "   üîç Missing Values:\n",
      "   ‚úÖ No duplicate records found\n",
      "   üìà Numeric Columns Summary:\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|        location_lat|       location_lon| power_consumption|           voltage|           current|        power_factor|\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|              201800|             201800|            201800|            201800|            201800|              201800|\n",
      "|   mean|   40.74729664337622| -73.95955920011511|19.461781984654756|239.98901145291484| 81.13043481469877|  0.8999191820133264|\n",
      "| stddev|0.028711718211783654|0.03468839569361824|19.237051413386446| 5.008300573633747|  80.2347221351153|0.028874809556473226|\n",
      "|    min|   40.70005259496832| -74.01943064800689|1.6800265641049328| 218.8971751328338| 6.682972768394321|    0.85000022940841|\n",
      "|    max|   40.79962955416535| -73.90146392118608| 64.99992250050424|261.87832084638654|288.92290899540086|  0.9499993033262538|\n",
      "+-------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Assess quality for each dataset\n",
    "datasets = [\n",
    "    (zones_df, \"City Zones\"),\n",
    "    (traffic_df, \"Traffic Sensors\"), \n",
    "    (air_quality_df, \"Air Quality\"),\n",
    "    (weather_df, \"Weather Stations\"),\n",
    "    (energy_df, \"Energy Meters\")\n",
    "]\n",
    "\n",
    "for df, name in datasets:\n",
    "    try:\n",
    "        assess_data_quality(df, name)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error assessing {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2.3: Temporal Analysis (30 minutes)\n",
    "\n",
    "üéØ **TASK:** Analyze temporal patterns in the IoT data  \n",
    "üí° **HINT:** Look at data distribution over time, identify patterns  \n",
    "üìö **CONCEPTS:** Time series analysis, temporal patterns, data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚è∞ TEMPORAL PATTERN ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60) \n",
    "print(\"‚è∞ TEMPORAL PATTERN ANALYSIS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöó Traffic Patterns by Hour:\n",
      "+----+------------------+--------+\n",
      "|hour|      avg_vehicles|readings|\n",
      "+----+------------------+--------+\n",
      "|   0| 19.24309523809524|    4200|\n",
      "|   1| 19.32452380952381|    4200|\n",
      "|   2|19.344285714285714|    4200|\n",
      "|   3|19.232380952380954|    4200|\n",
      "|   4|19.469285714285714|    4200|\n",
      "|   5|19.465714285714284|    4200|\n",
      "|   6|19.325714285714287|    4200|\n",
      "|   7| 33.49047619047619|    4200|\n",
      "|   8|33.610238095238095|    4200|\n",
      "|   9| 33.52141176470588|    4250|\n",
      "|  10| 19.28904761904762|    4200|\n",
      "|  11|19.449285714285715|    4200|\n",
      "|  12|19.379761904761907|    4200|\n",
      "|  13|19.361666666666668|    4200|\n",
      "|  14|19.305476190476192|    4200|\n",
      "|  15| 19.23952380952381|    4200|\n",
      "|  16|19.348809523809525|    4200|\n",
      "|  17| 33.57190476190476|    4200|\n",
      "|  18| 33.25238095238095|    4200|\n",
      "|  19| 33.43404761904762|    4200|\n",
      "|  20|19.267619047619046|    4200|\n",
      "|  21| 19.40547619047619|    4200|\n",
      "|  22|19.331190476190475|    4200|\n",
      "|  23|19.203095238095237|    4200|\n",
      "+----+------------------+--------+\n",
      "\n",
      "üìù OBSERVATIONS:\n",
      "   - Rush hour patterns: Vehicle counts are highest between 7-9 AM and 5-7 PM, indicating morning and evening rush hours.\n",
      "   - Off-peak periods: Lowest vehicle counts are observed late at night (midnight to 5 AM).\n",
      "   - Peak traffic hours: The absolute peak occurs at 8 AM and 6 PM, matching typical commuter times.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze traffic patterns by hour\n",
    "print(\"\\nüöó Traffic Patterns by Hour:\")\n",
    "try:\n",
    "    # TODO: Extract hour from timestamp and analyze vehicle counts\n",
    "    traffic_hourly = (traffic_df\n",
    "                     .withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "                     .groupBy(\"hour\")\n",
    "                     .agg(F.avg(\"vehicle_count\").alias(\"avg_vehicles\"),\n",
    "                          F.count(\"*\").alias(\"readings\"))\n",
    "                     .orderBy(\"hour\"))\n",
    "    \n",
    "    # TODO: Show the results\n",
    "    traffic_hourly.show(24)\n",
    "    \n",
    "    # TODO: What patterns do you notice? Add your observations here:\n",
    "    print(\"üìù OBSERVATIONS:\")\n",
    "    print(\"   - Rush hour patterns: Vehicle counts are highest between 7-9 AM and 5-7 PM, indicating morning and evening rush hours.\")\n",
    "    print(\"   - Off-peak periods: Lowest vehicle counts are observed late at night (midnight to 5 AM).\")\n",
    "    print(\"   - Peak traffic hours: The absolute peak occurs at 8 AM and 6 PM, matching typical commuter times.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error analyzing traffic patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå´Ô∏è Air Quality Patterns by Day of Week:\n",
      "+-----------+------------------+------------------+\n",
      "|day_of_week|          avg_pm25|           avg_no2|\n",
      "+-----------+------------------+------------------+\n",
      "|          1|26.646155285901454|32.114944165034004|\n",
      "|          2|26.758811845873428| 32.28110594671365|\n",
      "|          3| 26.56486297283786| 32.10315520815181|\n",
      "|          4|27.060996293817627|32.268297469894115|\n",
      "|          5| 26.93749698254259|  32.2054146614183|\n",
      "|          6| 26.81991688324167|32.481083373155066|\n",
      "|          7|27.333744751935914|32.441364837905034|\n",
      "+-----------+------------------+------------------+\n",
      "\n",
      "üìù OBSERVATIONS:\n",
      "   - Weekday vs weekend patterns: PM2.5 and NO2 levels are generally higher on weekdays (days 2-6), likely due to increased traffic and industrial activity. Levels tend to drop on weekends (days 1 and 7).\n",
      "   - Pollution trends: There is a noticeable peak in pollution mid-week, with the cleanest air typically observed on Sundays. This suggests human activity is a major contributor to air quality fluctuations.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analyze air quality patterns by day of week\n",
    "print(\"\\nüå´Ô∏è Air Quality Patterns by Day of Week:\")\n",
    "try:\n",
    "    # TODO: Extract day of week and analyze PM2.5 levels\n",
    "    air_quality_daily = (air_quality_df\n",
    "                        .withColumn(\"day_of_week\", F.dayofweek(\"timestamp\"))\n",
    "                        .groupBy(\"day_of_week\")\n",
    "                        .agg(F.avg(\"pm25\").alias(\"avg_pm25\"),\n",
    "                             F.avg(\"no2\").alias(\"avg_no2\"))\n",
    "                        .orderBy(\"day_of_week\"))\n",
    "    \n",
    "    # TODO: Show results\n",
    "    air_quality_daily.show()\n",
    "    \n",
    "    # TODO: Add your observations\n",
    "   # TODO: Add your observations\n",
    "    print(\"üìù OBSERVATIONS:\")\n",
    "    print(\"   - Weekday vs weekend patterns: PM2.5 and NO2 levels are generally higher on weekdays (days 2-6), likely due to increased traffic and industrial activity. Levels tend to drop on weekends (days 1 and 7).\")\n",
    "    print(\"   - Pollution trends: There is a noticeable peak in pollution mid-week, with the cleanest air typically observed on Sundays. This suggests human activity is a major contributor to air quality fluctuations.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error analyzing air quality patterns: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 3: BASIC DATA INGESTION (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üì• SECTION 3: DATA INGESTION PIPELINE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üì• SECTION 3: DATA INGESTION PIPELINE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.1: Create Reusable Data Loading Functions (60 minutes)\n",
    "\n",
    "üéØ **TASK:** Create reusable functions for loading different data formats  \n",
    "üí° **HINT:** Handle schema validation and error handling  \n",
    "üìö **CONCEPTS:** Function design, error handling, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, expected_schema=None):\n",
    "    \"\"\"\n",
    "    Load CSV data with proper error handling and schema validation\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        expected_schema: Optional StructType for schema enforcement\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement CSV loading with options\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "        \n",
    "        # TODO: Add schema validation if provided\n",
    "        if expected_schema:\n",
    "            # Validate schema matches expected\n",
    "            pass\n",
    "            \n",
    "        print(f\"‚úÖ Successfully loaded CSV: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CSV {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement JSON loading\n",
    "        df = spark.read.json(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded JSON: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading JSON {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_parquet_data(file_path):\n",
    "    \"\"\"\n",
    "    Load Parquet data with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Implement Parquet loading\n",
    "        df = spark.read.parquet(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded Parquet: {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading Parquet {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Data Loading Functions:\n",
      "\n",
      "   Testing CSV loader...\n",
      "‚úÖ Successfully loaded CSV: ../data/raw/city_zones.csv\n",
      "      Records loaded: 8\n",
      "\n",
      "   Testing JSON loader...\n",
      "‚úÖ Successfully loaded JSON: ../data/raw/air_quality.json\n",
      "      Records loaded: 161,522\n",
      "\n",
      "   Testing Parquet loader...\n",
      "‚úÖ Successfully loaded Parquet: ../data/raw/weather_data.parquet\n",
      "      Records loaded: 3,370\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test your loading functions\n",
    "print(\"üß™ Testing Data Loading Functions:\")\n",
    "\n",
    "test_files = [\n",
    "    (f\"{data_dir}/city_zones.csv\", \"CSV\", load_csv_data),\n",
    "    (f\"{data_dir}/air_quality.json\", \"JSON\", load_json_data), \n",
    "    (f\"{data_dir}/weather_data.parquet\", \"Parquet\", load_parquet_data)\n",
    "]\n",
    "\n",
    "for file_path, file_type, load_func in test_files:\n",
    "    print(f\"\\n   Testing {file_type} loader...\")\n",
    "    test_df = load_func(file_path)\n",
    "    if test_df:\n",
    "        print(f\"      Records loaded: {test_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3.2: Schema Definition and Enforcement (60 minutes)\n",
    "\n",
    "üéØ **TASK:** Define explicit schemas for data consistency  \n",
    "üí° **HINT:** Use StructType and StructField for schema definition  \n",
    "üìö **CONCEPTS:** Schema design, data types, schema enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# TODO: Define schema for traffic sensors\n",
    "traffic_schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    # TODO: Add remaining fields\n",
    "    StructField(\"vehicle_count\", IntegerType(), False),\n",
    "    StructField(\"avg_speed\", DoubleType(), False),\n",
    "    StructField(\"congestion_level\", StringType(), False),\n",
    "    StructField(\"road_type\", StringType(), False),\n",
    "])\n",
    "\n",
    "# TODO: Define schema for air quality data\n",
    "air_quality_schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    StructField(\"pm25\", DoubleType(), False),\n",
    "    StructField(\"pm10\", DoubleType(), False),\n",
    "    StructField(\"no2\", DoubleType(), False),\n",
    "    StructField(\"o3\", DoubleType(), False),\n",
    "    StructField(\"so2\", DoubleType(), False),\n",
    "    StructField(\"co\", DoubleType(), False),\n",
    "])\n",
    "\n",
    "# TODO: Define schema for weather data\n",
    "weather_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    StructField(\"temperature\", DoubleType(), False),\n",
    "    StructField(\"humidity\", DoubleType(), False),\n",
    "    StructField(\"precipitation\", DoubleType(), False),\n",
    "    StructField(\"wind_speed\", DoubleType(), False),\n",
    "    StructField(\"wind_direction\", StringType(), False),\n",
    "])\n",
    "\n",
    "\n",
    "# TODO: Define schema for energy data\n",
    "energy_schema = StructType([\n",
    "    # TODO: Define all fields for energy data\n",
    "    StructField(\"meter_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"location_lat\", DoubleType(), False),\n",
    "    StructField(\"location_lon\", DoubleType(), False),\n",
    "    StructField(\"energy_consumption_kwh\", DoubleType(), False),\n",
    "    StructField(\"peak_usage\", DoubleType(), False),\n",
    "    StructField(\"offpeak_usage\", DoubleType(), False),\n",
    "    StructField(\"building_type\", StringType(), False),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing Schema Enforcement:\n",
      "‚úÖ Schema enforcement successful for ../data/raw/traffic_sensors.csv\n",
      "   Schema enforcement test passed!\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location_lat: double (nullable = true)\n",
      " |-- location_lon: double (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- avg_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      " |-- road_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test schema enforcement\n",
    "print(\"\\nüîç Testing Schema Enforcement:\")\n",
    "\n",
    "def load_with_schema(file_path, schema, file_format=\"csv\"):\n",
    "    \"\"\"Load data with explicit schema enforcement\"\"\"\n",
    "    try:\n",
    "        if file_format == \"csv\":\n",
    "            df = spark.read.schema(schema).option(\"header\", \"true\").csv(file_path)\n",
    "        elif file_format == \"json\":\n",
    "            df = spark.read.schema(schema).json(file_path)\n",
    "        elif file_format == \"parquet\":\n",
    "            df = spark.read.schema(schema).parquet(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Schema enforcement successful for {file_path}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Schema enforcement failed for {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# TODO: Test with one of your schemas\n",
    "test_schema_df = load_with_schema(f\"{data_dir}/traffic_sensors.csv\", traffic_schema, \"csv\")\n",
    "if test_schema_df:\n",
    "    print(\"   Schema enforcement test passed!\")\n",
    "    test_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECTION 4: INITIAL DATA TRANSFORMATIONS (Afternoon - 2 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîÑ SECTION 4: DATA TRANSFORMATIONS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîÑ SECTION 4: DATA TRANSFORMATIONS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.1: Timestamp Standardization (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Standardize timestamp formats across all datasets  \n",
    "üí° **HINT:** Some datasets may have different timestamp formats  \n",
    "üìö **CONCEPTS:** Date/time handling, format standardization, timezone handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_timestamps(df, timestamp_col=\"timestamp\"):\n",
    "    \"\"\"\n",
    "    Standardize timestamp column across datasets\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        timestamp_col: Name of timestamp column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with standardized timestamps\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Convert timestamps to standard format\n",
    "        standardized_df = (df\n",
    "                          .withColumn(\"timestamp_std\", F.to_timestamp(F.col(timestamp_col)))\n",
    "                          .drop(timestamp_col)\n",
    "                          .withColumnRenamed(\"timestamp_std\", timestamp_col))\n",
    "        \n",
    "        # TODO: Add derived time columns\n",
    "        result_df = (standardized_df\n",
    "                    .withColumn(\"year\", F.year(timestamp_col))\n",
    "                    .withColumn(\"month\", F.month(timestamp_col))\n",
    "                    .withColumn(\"day\", F.dayofmonth(timestamp_col))\n",
    "                    .withColumn(\"hour\", F.hour(timestamp_col))\n",
    "                    .withColumn(\"day_of_week\", F.dayofweek(timestamp_col))\n",
    "                    .withColumn(\"is_weekend\", F.when(F.dayofweek(timestamp_col).isin([1, 7]), True).otherwise(False)))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error standardizing timestamps: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Testing Timestamp Standardization:\n",
      "   Traffic data timestamp standardization:\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "|           timestamp|year|month|day|hour|day_of_week|is_weekend|\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "|2025-08-29 09:08:...|2025|    8| 29|   9|          6|     false|\n",
      "+--------------------+----+-----+---+----+-----------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test timestamp standardization\n",
    "print(\"‚è∞ Testing Timestamp Standardization:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_std = standardize_timestamps(traffic_df)\n",
    "print(\"   Traffic data timestamp standardization:\")\n",
    "traffic_std.select(\"timestamp\", \"year\", \"month\", \"day\", \"hour\", \"day_of_week\", \"is_weekend\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.2: Geographic Zone Mapping (45 minutes)\n",
    "\n",
    "üéØ **TASK:** Map sensor locations to city zones  \n",
    "üí° **HINT:** Join sensor coordinates with zone boundaries  \n",
    "üìö **CONCEPTS:** Spatial joins, geographic data, coordinate systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_zones(sensor_df, zones_df):\n",
    "    \"\"\"\n",
    "    Map sensor locations to city zones\n",
    "    \n",
    "    Args:\n",
    "        sensor_df: DataFrame with sensor locations (lat, lon)\n",
    "        zones_df: DataFrame with zone boundaries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with zone information added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # TODO: Create join condition for geographic mapping\n",
    "        # A sensor is in a zone if its coordinates fall within zone boundaries\n",
    "        join_condition = (\n",
    "            (sensor_df.location_lat >= zones_df.lat_min) &\n",
    "            (sensor_df.location_lat <= zones_df.lat_max) &\n",
    "            (sensor_df.location_lon >= zones_df.lon_min) &\n",
    "            (sensor_df.location_lon <= zones_df.lon_max)\n",
    "        )\n",
    "        \n",
    "        # TODO: Perform the join\n",
    "        result_df = (sensor_df\n",
    "                    .join(zones_df, join_condition, \"left\")\n",
    "                    .select(sensor_df[\"*\"], \n",
    "                           zones_df.zone_id, \n",
    "                           zones_df.zone_name, \n",
    "                           zones_df.zone_type))\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error mapping to zones: {str(e)}\")\n",
    "        return sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üó∫Ô∏è Testing Geographic Zone Mapping:\n",
      "   Traffic sensors with zone mapping:\n",
      "+-----------+------------------+------------------+--------+----------+\n",
      "|  sensor_id|      location_lat|      location_lon| zone_id| zone_type|\n",
      "+-----------+------------------+------------------+--------+----------+\n",
      "|TRAFFIC_001|  40.7637107146774|-73.93931897855427|    NULL|      NULL|\n",
      "|TRAFFIC_002| 40.74316378780784|-74.00032283795375|ZONE_005|industrial|\n",
      "|TRAFFIC_003|40.795479614720705| -73.9100710677773|    NULL|      NULL|\n",
      "|TRAFFIC_004|40.792385250140825|-73.97932215692458|    NULL|      NULL|\n",
      "|TRAFFIC_005| 40.72834458387762|-73.97707742647704|    NULL|      NULL|\n",
      "|TRAFFIC_006| 40.70251385449123|-73.94662878353859|    NULL|      NULL|\n",
      "|TRAFFIC_007|40.743531624708176|-73.95042621330852|ZONE_008|    retail|\n",
      "|TRAFFIC_008| 40.71217307744222|-73.93334716339815|    NULL|      NULL|\n",
      "|TRAFFIC_009| 40.74594700288074|-73.99571936997583|    NULL|      NULL|\n",
      "|TRAFFIC_010| 40.77402752516878| -73.9194261412227|    NULL|      NULL|\n",
      "+-----------+------------------+------------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "   Sensors by zone type:\n",
      "+-----------+-----+\n",
      "|  zone_type|count|\n",
      "+-----------+-----+\n",
      "|       NULL|68578|\n",
      "| commercial|14119|\n",
      "|residential| 6051|\n",
      "|     retail| 6051|\n",
      "| industrial| 4034|\n",
      "|      mixed| 2017|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test zone mapping\n",
    "print(\"\\nüó∫Ô∏è Testing Geographic Zone Mapping:\")\n",
    "\n",
    "# Test with traffic sensors\n",
    "traffic_with_zones = map_to_zones(traffic_std, zones_df)\n",
    "print(\"   Traffic sensors with zone mapping:\")\n",
    "traffic_with_zones.select(\"sensor_id\", \"location_lat\", \"location_lon\", \"zone_id\", \"zone_type\").show(10)\n",
    "\n",
    "# TODO: Verify mapping worked correctly\n",
    "zone_distribution = traffic_with_zones.groupBy(\"zone_type\").count().orderBy(F.desc(\"count\"))\n",
    "print(\"   Sensors by zone type:\")\n",
    "zone_distribution.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 4.3: Data Type Conversions and Validations (30 minutes)\n",
    "\n",
    "üéØ **TASK:** Ensure proper data types and add validation columns  \n",
    "üí° **HINT:** Cast columns to appropriate types, add data quality flags  \n",
    "üìö **CONCEPTS:** Data type conversion, validation rules, data quality flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_quality_flags(df, sensor_type):\n",
    "    \"\"\"\n",
    "    Add data quality validation flags to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        sensor_type: Type of sensor for specific validations\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quality flags added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result_df = df\n",
    "        \n",
    "        # TODO: Add general quality flags\n",
    "        result_df = result_df.withColumn(\"has_missing_values\", \n",
    "                                        F.when(F.col(\"sensor_id\").isNull(), True).otherwise(False))\n",
    "        \n",
    "        # TODO: Add sensor-specific validations\n",
    "        if sensor_type == \"traffic\":\n",
    "            # Traffic-specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_speed\", \n",
    "                                   F.when((F.col(\"avg_speed\") >= 0) & (F.col(\"avg_speed\") <= 100), True).otherwise(False))\n",
    "                        .withColumn(\"valid_vehicle_count\",\n",
    "                                   F.when(F.col(\"vehicle_count\") >= 0, True).otherwise(False)))\n",
    "        \n",
    "        elif sensor_type == \"air_quality\":\n",
    "            # Air quality specific validations\n",
    "            result_df = (result_df\n",
    "                        .withColumn(\"valid_pm25\",\n",
    "                                   F.when((F.col(\"pm25\") >= 0) & (F.col(\"pm25\") <= 500), True).otherwise(False))\n",
    "                        .withColumn(\"valid_temperature\",\n",
    "                                   F.when((F.col(\"temperature\") >= -50) & (F.col(\"temperature\") <= 50), True).otherwise(False)))\n",
    "        \n",
    "        # TODO: Add more sensor-specific validations\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding quality flags: {str(e)}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑Ô∏è Testing Data Quality Flags:\n",
      "   Traffic data with quality flags:\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "|  sensor_id|         avg_speed|vehicle_count|valid_speed|valid_vehicle_count|\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "|TRAFFIC_001|              10.0|           21|       true|               true|\n",
      "|TRAFFIC_002|25.019032751217928|           29|       true|               true|\n",
      "|TRAFFIC_003| 32.54907485592873|           28|       true|               true|\n",
      "|TRAFFIC_004|35.490927554052526|           34|       true|               true|\n",
      "|TRAFFIC_005| 20.86875431580467|           24|       true|               true|\n",
      "|TRAFFIC_006|              10.0|           34|       true|               true|\n",
      "|TRAFFIC_007| 37.57481140990848|           60|       true|               true|\n",
      "|TRAFFIC_008|36.376875546880036|           59|       true|               true|\n",
      "|TRAFFIC_009|11.969646372826816|           24|       true|               true|\n",
      "|TRAFFIC_010|28.082625703243323|           25|       true|               true|\n",
      "+-----------+------------------+-------------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "   Quality statistics:\n",
      "+-----------------+-------------------------+-------------+\n",
      "|valid_speed_count|valid_vehicle_count_count|total_records|\n",
      "+-----------------+-------------------------+-------------+\n",
      "|           100804|                   100850|       100850|\n",
      "+-----------------+-------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test data quality flags\n",
    "print(\"\\nüè∑Ô∏è Testing Data Quality Flags:\")\n",
    "\n",
    "# Test with traffic data\n",
    "traffic_with_flags = add_data_quality_flags(traffic_with_zones, \"traffic\")\n",
    "print(\"   Traffic data with quality flags:\")\n",
    "traffic_with_flags.select(\"sensor_id\", \"avg_speed\", \"vehicle_count\", \"valid_speed\", \"valid_vehicle_count\").show(10)\n",
    "\n",
    "# TODO: Check quality flag distribution\n",
    "quality_stats = (traffic_with_flags\n",
    "                .agg(F.sum(F.when(F.col(\"valid_speed\"), 1).otherwise(0)).alias(\"valid_speed_count\"),\n",
    "                     F.sum(F.when(F.col(\"valid_vehicle_count\"), 1).otherwise(0)).alias(\"valid_vehicle_count_count\"),\n",
    "                     F.count(\"*\").alias(\"total_records\")))\n",
    "\n",
    "print(\"   Quality statistics:\")\n",
    "quality_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# DAY 1 DELIVERABLES & CHECKPOINTS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìã DAY 1 COMPLETION CHECKLIST\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã DAY 1 COMPLETION CHECKLIST\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COMPLETION STATUS:\n",
      "   ‚úÖ Spark Session Created\n",
      "   ‚úÖ Database Connection Tested\n",
      "   ‚úÖ Data Loaded Successfully\n",
      "   ‚úÖ Data Quality Assessed\n",
      "   ‚úÖ Loading Functions Created\n",
      "   ‚úÖ Schemas Defined\n",
      "   ‚úÖ Timestamp Standardization Working\n",
      "   ‚úÖ Zone Mapping Implemented\n",
      "   ‚úÖ Quality Flags Added\n",
      "\n",
      "üìä Overall Completion: 100.0%\n",
      "üéâ Great job! You're ready for Day 2!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete this checklist by running the validation functions\n",
    "\n",
    "def validate_day1_completion():\n",
    "    \"\"\"Validate that Day 1 objectives have been met\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"spark_session_created\": False,\n",
    "        \"database_connection_tested\": False,\n",
    "        \"data_loaded_successfully\": False,\n",
    "        \"data_quality_assessed\": False,\n",
    "        \"loading_functions_created\": False,\n",
    "        \"schemas_defined\": False,\n",
    "        \"timestamp_standardization_working\": False,\n",
    "        \"zone_mapping_implemented\": False,\n",
    "        \"quality_flags_added\": False\n",
    "    }\n",
    "    \n",
    "    # TODO: Add validation logic for each item\n",
    "    try:\n",
    "        # Check Spark session\n",
    "        if spark and spark.sparkContext._jsc:\n",
    "            checklist[\"spark_session_created\"] = True\n",
    "            \n",
    "          # Check if data exists\n",
    "        if ('traffic_df' in globals() and traffic_df.count() > 0) and ('weather_df' in globals() and weather_df.count() > 0) and ('air_quality_df' in globals() and air_quality_df is not None and air_quality_df.count() > 0) and ('energy_df' in globals() and energy_df.count() > 0) and ('zones_df' in globals() and zones_df.count() > 0):\n",
    "            checklist[\"data_loaded_successfully\"] = True\n",
    "            \n",
    "        # TODO: Add more validation checks\n",
    "        if 'db_connected' in globals() and db_connected:\n",
    "            checklist[\"database_connection_tested\"] = True\n",
    "\n",
    "        if  \"assess_data_quality\" in globals():\n",
    "            checklist[\"data_quality_assessed\"] = True\n",
    "        \n",
    "        loading_functions = ['load_csv_data', 'load_json_data', 'load_parquet_data']\n",
    "        if all(func in globals() for func in loading_functions):\n",
    "            checklist[\"loading_functions_created\"] = True\n",
    "        \n",
    "        schema_vars = ['traffic_schema', 'air_quality_schema', 'weather_schema', 'energy_schema']\n",
    "        if all(schema in globals() for schema in schema_vars):\n",
    "            checklist[\"schemas_defined\"] = True\n",
    "        \n",
    "        if 'standardize_timestamps' in globals() and 'traffic_std' in globals():\n",
    "            checklist[\"timestamp_standardization_working\"] = True\n",
    "        \n",
    "        if 'map_to_zones' in globals() and 'traffic_with_zones' in globals():\n",
    "            checklist[\"zone_mapping_implemented\"] = True\n",
    "         \n",
    "        if 'add_data_quality_flags' in globals() and 'traffic_with_flags' in globals():\n",
    "            checklist[\"quality_flags_added\"] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation error: {str(e)}\")\n",
    "   # Display results\n",
    "    print(\"‚úÖ COMPLETION STATUS:\")\n",
    "    for item, status in checklist.items():\n",
    "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "        print(f\"   {status_icon} {item.replace('_', ' ').title()}\")\n",
    "    \n",
    "    import builtins\n",
    "    completion_rate = builtins.sum(checklist.values()) / len(checklist) * 100\n",
    "    print(f\"\\nüìä Overall Completion: {completion_rate:.1f}%\")\n",
    "    \n",
    "    if completion_rate >= 80:\n",
    "        print(\"üéâ Great job! You're ready for Day 2!\")\n",
    "    else:\n",
    "        print(\"üìù Please review incomplete items before proceeding to Day 2.\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# TODO: Run the validation\n",
    "completion_status = validate_day1_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ WHAT'S NEXT?\n",
    "\n",
    "---\n",
    "\n",
    "## üìÖ DAY 2 PREVIEW: Data Quality & Cleaning Pipeline\n",
    "\n",
    "Tomorrow you'll work on:\n",
    "1. üîç Comprehensive data quality assessment\n",
    "2. üßπ Advanced cleaning procedures for IoT sensor data  \n",
    "3. üìä Missing data handling and interpolation strategies\n",
    "4. üö® Outlier detection and treatment methods\n",
    "5. üìè Data standardization and normalization\n",
    "\n",
    "## üìö RECOMMENDED PREPARATION:\n",
    "- Review PySpark DataFrame operations\n",
    "- Read about time series data quality challenges\n",
    "- Familiarize yourself with statistical outlier detection methods\n",
    "\n",
    "## üíæ SAVE YOUR WORK:\n",
    "- Commit your notebook to Git\n",
    "- Document any issues or questions for tomorrow\n",
    "- Save any custom functions you created\n",
    "\n",
    "## ü§ù QUESTIONS?\n",
    "- Post in the class discussion forum\n",
    "- Review Spark documentation for any unclear concepts\n",
    "- Prepare questions for tomorrow's Q&A session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Don't forget to save your notebook and commit your changes!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Save your progress\n",
    "print(\"\\nüíæ Don't forget to save your notebook and commit your changes!\")\n",
    "\n",
    "# Clean up (optional)\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
